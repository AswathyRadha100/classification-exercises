{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d86d6fe",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; padding: 40px; font-size: 40px;\">\n",
    "             Classification Model Evaluation\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb5bb0",
   "metadata": {},
   "source": [
    "   Classification is a technique for labeling the class of an observation.Classification can be used to predict        binary classes (2 classes) or multi-classes (>2 classes).\n",
    "\n",
    "   Classification Model: A series of steps that takes the patterns of input variables, generalizes those patterns,    and applies them to new data in order to predict the class.\n",
    "   \n",
    "   A confusion matrix is a cross-tabulation of our model's predictions against the actual values. \n",
    "  \n",
    "   Accuracy evaluates how many correct predictions (both positive and negative) were made over the total number of    predictions.\n",
    "  \n",
    "   Precision evaluates how many of the positive predictions were correct.\n",
    "  \n",
    "   Recall evaluates how the model handled all positive outcomes.\n",
    "  \n",
    "   Misclassification rate concerns how many predictions were incorrect. This accounts for all other outcomes not      included in the calculation of accuracy.\n",
    "  \n",
    "   Sensitivity (True Positive Rate) TP/(TP+FN)\n",
    "\n",
    "   How well does the model predict negative outcomes? Specificity TN/(FP+TN)\n",
    "\n",
    "   Negative Predictive Value  TN/(FN+TN)\n",
    "  \n",
    "   F1 Score  2*((Precision*Recall)/(Precision+Recall))\n",
    "\n",
    "   The baseline is a simple model that is a reference point for the performance of other models.\n",
    "   For a classification model, a baseline is often the mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5274b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Designation\tDescription\n",
    "True Negative\tModel correctly predicted the negative outcome\n",
    "False Positive\tModel incorrectly predicted the positive outcome\n",
    "False Negative\tModel incorrectly predicted the negative outcome\n",
    "True Positive\tModel correctly predicted the positive outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad97cd8",
   "metadata": {},
   "source": [
    "### Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c32dbf",
   "metadata": {},
   "source": [
    "### 1. Create a new file named model_evaluation.py or model_evaluation.ipynb for these exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e48b3f1",
   "metadata": {},
   "source": [
    "### 2.Given the following confusion matrix, evaluate (by hand) the model's performance.\n",
    "\n",
    "\n",
    "|               | pred dog   | pred cat   |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| actual dog    |         46 |         7  |\n",
    "| actual cat    |         13 |         34 |\n",
    "In the context of this problem, what is a false positive?\n",
    "In the context of this problem, what is a false negative?\n",
    "How would you describe this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e96c77",
   "metadata": {},
   "source": [
    "Dog is negative class\n",
    "Cat is positive class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be713bf",
   "metadata": {},
   "source": [
    "### 2.1 In the context of this problem, what is a false positive? \n",
    "\n",
    "False Positive: We predicted a cat but it is a dog.\n",
    "\n",
    "The predicted false positive is 7.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84df2339",
   "metadata": {},
   "source": [
    "### 2.2 In the context of this problem, what is a false negative?\n",
    "\n",
    "False Negative: We predicted a dog but it is a cat.\n",
    "\n",
    "The predicted false negative is 13.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5d5206",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### 2.3 How would you describe this model?\n",
    "\n",
    "Since we are setting Cat is positive class and Dog is negative class:\n",
    "\n",
    "\n",
    "True positive would be predicting a cat and it is an actual cat\n",
    "\n",
    "True negative would be predicting a dog and it is an actual dog\n",
    "\n",
    "False positive would be predicting a cat and it is a dog\n",
    "\n",
    "False negative would be predicting a dog and it is a cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "17f7fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#true positive is predicting its a cat, and its a cat\n",
    "tp = 34\n",
    "\n",
    "#true negative is predicting its a dog, and its a dog\n",
    "tn = 46\n",
    "\n",
    "#false positive is predicting its a cat, but its a dog\n",
    "fp = 7\n",
    "\n",
    "#false negative is predicting its a dog, but its a cat\n",
    "fn = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29e3359c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat-classifier (i.e 'cat' is the positive prediction)\n",
      "False Negatives: 13\n",
      "True Negatives: 46\n",
      "True Positives: 34\n",
      "False Positives: 7\n",
      "===========================\n",
      "Accuracy is: 0.8\n",
      "Recall is: 0.72\n",
      "Precision is: 0.83\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Cat-classifier (i.e 'cat' is the positive prediction)\")\n",
    "\n",
    "print(\"False Negatives:\", fn)\n",
    "print(\"True Negatives:\", tn)\n",
    "print(\"True Positives:\", tp)\n",
    "print(\"False Positives:\", fp)\n",
    "print(\"===========================\")\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "print(\"Accuracy is:\", accuracy)\n",
    "print(\"Recall is:\", round(recall,2))\n",
    "print(\"Precision is:\", round(precision,2))      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d2fd12",
   "metadata": {},
   "source": [
    "### 3. You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant.\n",
    "\n",
    "Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects, and the data from their predictions can be found here.\n",
    "\n",
    "Use the predictions dataset and pandas to help answer the following questions:\n",
    "\n",
    "An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?\n",
    "\n",
    "\n",
    "Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "90ac9df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e509c885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual     model1  model2     model3\n",
       "0  No Defect  No Defect  Defect  No Defect\n",
       "1  No Defect  No Defect  Defect     Defect\n",
       "2  No Defect  No Defect  Defect  No Defect\n",
       "3  No Defect     Defect  Defect     Defect\n",
       "4  No Defect  No Defect  Defect  No Defect"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the c3.csv into your current folder\n",
    "#load dataframe\n",
    "ccc_df = pd.read_csv('c3.csv')\n",
    "\n",
    "#take a look\n",
    "ccc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345d0605",
   "metadata": {},
   "source": [
    "### 3. a. An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4a3a2289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actual\n",
       "No Defect    184\n",
       "Defect        16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the no: of defects and non-defects in the actual data\n",
    "ccc_df.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6d8e00fa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>model1</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Defect</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Defect</th>\n",
       "      <td>2</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model1     Defect  No Defect\n",
       "actual                      \n",
       "Defect          8          8\n",
       "No Defect       2        182"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparing the distribution of categorical variables in the dataset for model1\n",
    "pd.crosstab(ccc_df.actual, ccc_df.model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "edfef64f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>model2</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Defect</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Defect</th>\n",
       "      <td>81</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model2     Defect  No Defect\n",
       "actual                      \n",
       "Defect          9          7\n",
       "No Defect      81        103"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparing the distribution of categorical variables in the dataset for model2\n",
    "pd.crosstab(ccc_df.actual, ccc_df.model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "395f7e2c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>model3</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Defect</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Defect</th>\n",
       "      <td>86</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model3     Defect  No Defect\n",
       "actual                      \n",
       "Defect         13          3\n",
       "No Defect      86         98"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparing the distribution of categorical variables in the dataset for model3\n",
    "pd.crosstab(ccc_df.actual, ccc_df.model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd915b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "As we are interested in defect we will assign it as positive class for the classifier.\n",
    "\n",
    "defects = positive class\n",
    "Our best metric here is recall = tp/(tp + fn)\n",
    "\n",
    "how many real positives do we have?\n",
    "how many of defective ducks are actually flagged by defective (positive) by the models?\n",
    "let's minimize our false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ecfa9ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    actual     model1     model2  model3\n",
       "13  Defect  No Defect     Defect  Defect\n",
       "30  Defect     Defect  No Defect  Defect\n",
       "65  Defect     Defect     Defect  Defect\n",
       "70  Defect     Defect     Defect  Defect\n",
       "74  Defect  No Defect  No Defect  Defect"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model positives\n",
    "subset = ccc_df [ccc_df.actual == 'Defect']\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e8038ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Recall: 50.00% \n",
      " Model 2 Recall: 56.25% \n",
      " Model 3 Recall: 81.25% \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nChoose Recall as the metric and Model 3 as the optimal model\\n\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model1\n",
    "model1_recall = (subset['actual'] == subset['model1']).mean()\n",
    "\n",
    "# model2\n",
    "model2_recall = (subset['actual'] == subset['model2']).mean()\n",
    "#model3\n",
    "model3_recall = (subset['actual'] == subset['model3']).mean()\n",
    "\n",
    "print(f\"Model 1 Recall: {model1_recall:.2%} \\n Model 2 Recall: {model2_recall:.2%} \\n Model 3 Recall: {model3_recall:.2%} \\n\")\n",
    "\n",
    "'''\n",
    "Choose Recall as the metric and Model 3 as the optimal model\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff927113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1d528d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model 1 recall\n",
    "(subset.actual == subset.model1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "71d58aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5625"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model 2 recall\n",
    "(subset.actual == subset.model2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "52117def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model 3 recall\n",
    "(subset.actual == subset.model3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569324c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Quality Control should select a model with higher recall (to avoid false negatives)\n",
    "Quality Control should use Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91199e01",
   "metadata": {},
   "source": [
    "### b. Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "319065a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Precision: 80.00%\n",
      "Model 2 Precision: 10.00%\n",
      "Model 3 Precision: 13.13%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pull positive predictions from each model\n",
    "# subset the data for positives from each model\n",
    "\n",
    "#model1 precision\n",
    "subset = ccc_df[ccc_df.model1=='Defect']\n",
    "model1_precision = (subset.actual == subset.model1).mean()\n",
    "\n",
    "# choose subset for model2 where we only select 'positive predictions'\n",
    "subset2 = ccc_df [ccc_df.model2 == 'Defect']\n",
    "# calculate precision\n",
    "model2_precision = (subset2.actual == subset2.model2).mean()\n",
    "\n",
    "# choose subset for model3 where we only select 'positive predictions'\n",
    "subset3 = ccc_df [ccc_df.model3 == 'Defect']\n",
    "# calculate precision\n",
    "model3_precision = (subset3.actual == subset3.model3).mean()\n",
    "\n",
    "print(f\"\"\"Model 1 Precision: {model1_precision:.2%}\n",
    "Model 2 Precision: {model2_precision:.2%}\n",
    "Model 3 Precision: {model3_precision:.2%}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e758bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PR team really wants to minimize the False positives means choose the model with the highest precision.\n",
    "\n",
    "So the best models for this scenario is precision = tp / (tp + fp)\n",
    "\n",
    "defect = positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "22a2eed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose subset for model 1 where we only select 'positive predictions'\n",
    "subset = ccc_df [ccc_df.model1 == 'Defect']\n",
    "\n",
    "# calculate precision\n",
    "(subset.actual == subset.model1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bfce761d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Choose subset for model2 where we only select 'positive predictions'\n",
    "subset = ccc_df[ccc_df.model2 == 'Defect']\n",
    "\n",
    "#calculate precision\n",
    "(subset.actual == subset.model2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6792bd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13131313131313133"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Choose subset for model3 where we only select 'positive predictions'\n",
    "subset = ccc_df[ccc_df.model3 == 'Defect']\n",
    "\n",
    "#calculate precision\n",
    "(subset.actual == subset.model3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a3ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "Use model 1 as it has the highest precision, hence it will minimize the false positive predictions of defects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb99c516",
   "metadata": {},
   "source": [
    "### 4.You are working as a data scientist for Gives You Paws ™, a subscription based service that shows you cute pictures of dogs or cats (or both for an additional fee).At Gives You Paws, anyone can upload pictures of their cats or dogs. The photos are then put through a two step process. First an automated algorithm tags pictures as either a cat or a dog (Phase I). Next, the photos that have been initially identified are put through another round of review, possibly with some human oversight, before being presented to the users (Phase II). Several models have already been developed with the data, and you can find their results here. Given this dataset, use pandas to create a baseline model (i.e. a model that just predicts the most common class) and answer the following questions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88811a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4\n",
       "0    cat    cat    dog    cat    dog\n",
       "1    dog    dog    cat    cat    dog\n",
       "2    dog    cat    cat    cat    dog\n",
       "3    dog    dog    dog    cat    dog\n",
       "4    cat    cat    cat    dog    dog"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download the gives_you_paws.csv into your current folder\n",
    "# load dataframe\n",
    "paws_df = pd.read_csv('gives_you_paws.csv')\n",
    "\n",
    "# take a look\n",
    "paws_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5163b3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   actual  5000 non-null   object\n",
      " 1   model1  5000 non-null   object\n",
      " 2   model2  5000 non-null   object\n",
      " 3   model3  5000 non-null   object\n",
      " 4   model4  5000 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 195.4+ KB\n"
     ]
    }
   ],
   "source": [
    "#Look at the kind of columns and dtypes we are dealing with\n",
    "paws_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ad0a9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actual\n",
       "dog    3254\n",
       "cat    1746\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our actual counts\n",
    "paws_df.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d344a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4 baseline\n",
       "0    cat    cat    dog    cat    dog      dog\n",
       "1    dog    dog    cat    cat    dog      dog\n",
       "2    dog    cat    cat    cat    dog      dog\n",
       "3    dog    dog    dog    cat    dog      dog\n",
       "4    cat    cat    cat    dog    dog      dog"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set the most common class ('dog') as the baseline\n",
    "paws_df['baseline'] = paws_df.actual.value_counts().idxmax()\n",
    "paws_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4740a3f",
   "metadata": {},
   "source": [
    "### 4. a. In terms of accuracy, how do the various models compare to the baseline model? Are any of the models better than the baseline?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7458e8c",
   "metadata": {},
   "source": [
    "\n",
    "cat = positive class\n",
    "dog = negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52ab71f4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline 0.6508\n",
      "Model1 0.8074\n",
      "Model2 0.6304\n",
      "Model3 0.5096\n",
      "Model4 0.7426\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline\",( paws_df['actual'] == paws_df['baseline']).mean())\n",
    "\n",
    "print(\"Model1\",( paws_df['actual'] == paws_df['model1']).mean())\n",
    "\n",
    "print(\"Model2\",( paws_df['actual'] == paws_df['model2']).mean())\n",
    "\n",
    "print(\"Model3\",( paws_df['actual'] == paws_df['model3']).mean())\n",
    "\n",
    "print(\"Model4\",( paws_df['actual'] == paws_df['model4']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb87c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f199d937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6508"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline accuracy \n",
    "(paws_df.actual == paws_df.baseline).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df11d7a8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8074"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model 1 accuracy\n",
    "(paws_df.model1 == paws_df.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c98d8bd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6304"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model 2 accuracy\n",
    "(paws_df.model2 == paws_df.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "914105c4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5096"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model 3 accuracy\n",
    "(paws_df.model3 == paws_df.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcff8c61",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7426"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model 4 accuracy\n",
    "(paws_df.model4 == paws_df.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e379e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "In terms of accuracy, model 1 and model 4 perform better than baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7846d7ac",
   "metadata": {},
   "source": [
    "### 4. b. Suppose you are working on a team that solely deals with dog pictures. Which of these models would you recommend?\n",
    "\n",
    "dog = positive class\n",
    "cat = negative class\n",
    "Two-phases - recall = tp/(tp + fn) and precision = tp/(tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting the data for just dogs\n",
    "# we are using reality, so use recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83e10e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4 baseline\n",
       "1    dog    dog    cat    cat    dog      dog\n",
       "2    dog    cat    cat    cat    dog      dog\n",
       "3    dog    dog    dog    cat    dog      dog\n",
       "5    dog    dog    dog    dog    dog      dog\n",
       "8    dog    dog    cat    dog    dog      dog"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Phase I, choose a model with highest recall\n",
    "\n",
    "subset = paws_df[paws_df.actual == 'dog']\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a5cfdf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['model1', 0.803318992009834],\n",
       " ['model2', 0.49078057775046097],\n",
       " ['model3', 0.5086047940995697],\n",
       " ['model4', 0.9557467732022127],\n",
       " ['baseline', 1.0]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_recall = []\n",
    "\n",
    "# for everything except actual so from model1 and on... do something\n",
    "for model in subset.columns[1:]:\n",
    "    recall = (subset.actual == subset[model]).mean()\n",
    "    model_recall.append([model, recall])\n",
    "    \n",
    "model_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb332d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c65c06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.803318992009834"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 1 Recall\n",
    "(subset.actual == subset.model1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbe274d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49078057775046097"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 2 Recall\n",
    "(subset.actual == subset.model2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58c836c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5086047940995697"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 3 Recall\n",
    "(subset.actual == subset.model3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b7fcc99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9557467732022127"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 4 Recall\n",
    "(subset.actual == subset.model4).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83e927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model 4 is performing the best, with Recall of 0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380bd6c5",
   "metadata": {},
   "source": [
    "### For Phase II, choose a model with highest precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7564c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset1 = paws_df[paws_df.model1 == 'dog']\n",
    "subset2 = paws_df[paws_df.model2 == 'dog']\n",
    "subset3 = paws_df[paws_df.model3 == 'dog']\n",
    "subset4 = paws_df[paws_df.model4 == 'dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42430b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset 1:  0.8900238338440586\n",
      "Subset 2:  0.8931767337807607\n",
      "Subset 3:  0.6598883572567783\n",
      "Subset 4:  0.7312485304490948\n"
     ]
    }
   ],
   "source": [
    "print(\"Subset 1: \", (subset1.actual == subset1.model1).mean())\n",
    "print(\"Subset 2: \", (subset2.actual == subset2.model2).mean())\n",
    "print(\"Subset 3: \", (subset3.actual == subset3.model3).mean())\n",
    "print(\"Subset 4: \", (subset4.actual == subset4.model4).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f26820",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model 2 performs the best in terms of our evaluation metric: Precision of 89.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52981b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0a1a80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4 baseline\n",
       "1    dog    dog    cat    cat    dog      dog\n",
       "2    dog    cat    cat    cat    dog      dog\n",
       "3    dog    dog    dog    cat    dog      dog\n",
       "5    dog    dog    dog    dog    dog      dog\n",
       "8    dog    dog    cat    dog    dog      dog"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Phase II, choose a model with highest precision\n",
    "\n",
    "subset = paws_df[paws_df.actual == 'dog']\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c71805b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4 baseline\n",
       "0    cat    cat    dog    cat    dog      dog\n",
       "1    dog    dog    cat    cat    dog      dog\n",
       "2    dog    cat    cat    cat    dog      dog\n",
       "3    dog    dog    dog    cat    dog      dog\n",
       "4    cat    cat    cat    dog    dog      dog"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take another look\n",
    "paws_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1484c414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset +ive classes in each model to calculate precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6bb2e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset1 = paws_df[paws_df.model1 == 'dog']\n",
    "subset2 = paws_df[paws_df.model2 == 'dog']\n",
    "subset3 = paws_df[paws_df.model3 == 'dog']\n",
    "subset4 = paws_df[paws_df.model4 == 'dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21f65b27",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8900238338440586"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 1 Precision\n",
    "(subset1.actual == subset1.model1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c505f497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8931767337807607"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 2 Precision\n",
    "(subset2.actual == subset2.model2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77d0abcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6598883572567783"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 3 Precision\n",
    "(subset3.actual == subset3.model3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "244a55a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7312485304490948"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 4 Precision\n",
    "(subset4.actual == subset4.model4).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cf60b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model 2 and Model 1 are performing best with Precision of 0.893"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17d100",
   "metadata": {},
   "source": [
    "### 4. c. Suppose you are working on a team that solely deals with cat pictures. Which of these models would you recommend?\n",
    "\n",
    "dog = negative class\n",
    "cat = postive class\n",
    "Two-phases - recall = tp/(tp + fn) and precision = tp/(tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f131ae3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4 baseline\n",
       "0    cat    cat    dog    cat    dog      cat\n",
       "1    dog    dog    cat    cat    dog      cat\n",
       "2    dog    cat    cat    cat    dog      cat\n",
       "3    dog    dog    dog    cat    dog      cat\n",
       "4    cat    cat    cat    dog    dog      cat"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#paws_df.drop(columns=['baseline'], inplace=True)\n",
    "paws_df['baseline'] = 'cat'\n",
    "paws_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb73c1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['model1', 0.6897721764420747],\n",
       " ['model2', 0.4841220423412204],\n",
       " ['model3', 0.358346709470305],\n",
       " ['model4', 0.8072289156626506],\n",
       " ['baseline', 0.3492]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#phaseII: precision\n",
    "\n",
    "model_prec = []\n",
    "\n",
    "\n",
    "for model in paws_df.columns[1:]:\n",
    "    subset = paws_df[paws_df[model] == 'cat']\n",
    "    \n",
    "    precision = (subset.actual == subset[model]).mean()\n",
    "    \n",
    "    \n",
    "    model_prec.append([model, precision])\n",
    "    \n",
    "model_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631fa631",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model 4 is best in terms of recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ff1868f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['model1', 0.6897721764420747],\n",
       " ['model2', 0.811332503113325],\n",
       " ['model3', 0.8117977528089888],\n",
       " ['model4', 0.8313253012048193],\n",
       " ['baseline', 0.8074]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Phase I recall\n",
    "\n",
    "subset = paws_df[paws_df.actual=='cat']\n",
    "\n",
    "model_recall = []\n",
    "\n",
    "\n",
    "for model in subset.columns[1:]:\n",
    "    subset = paws_df[paws_df[model] == 'cat']\n",
    "    \n",
    "    recall = (subset.actual == subset.model1).mean()\n",
    "\n",
    "    \n",
    "    model_recall.append([model, recall])\n",
    "    \n",
    "model_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model 4 is best in terms of Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d657a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e7e22c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual model1 model2 model3 model4 baseline\n",
       "0     cat    cat    dog    cat    dog      cat\n",
       "4     cat    cat    cat    dog    dog      cat\n",
       "6     cat    cat    cat    cat    dog      cat\n",
       "7     cat    dog    cat    cat    dog      cat\n",
       "11    cat    cat    dog    cat    cat      cat"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Phase I, choose a model with highest recall\n",
    "\n",
    "subset = paws_df[paws_df.actual == 'cat']\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "387a676f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8150057273768614"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 1 Recall\n",
    "(subset.actual == subset.model1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0cb7f548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8906071019473081"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 2 Recall\n",
    "(subset.actual == subset.model2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "763f7fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5114547537227949"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 3 Recall\n",
    "(subset.actual == subset.model3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e02f9a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34536082474226804"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 4 Recall\n",
    "(subset.actual == subset.model4).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model 2 is performing the best, with Recall of 0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "125dc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset1 = paws_df[paws_df.model1 == 'cat']\n",
    "subset2 = paws_df[paws_df.model2 == 'cat']\n",
    "subset3 = paws_df[paws_df.model3 == 'cat']\n",
    "subset4 = paws_df[paws_df.model4 == 'cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad940cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6897721764420747"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 1 Precision\n",
    "(subset1.actual == subset1.model1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45b28457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4841220423412204"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 2 Precision\n",
    "(subset2.actual == subset2.model2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b975da3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.358346709470305"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 3 Precision\n",
    "(subset3.actual == subset3.model3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3dcb5d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8072289156626506"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 4 Precision\n",
    "(subset4.actual == subset4.model4).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77591be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model 4 is performing the best, with precision of .81"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3044b812",
   "metadata": {},
   "source": [
    "### 5. Follow the links below to read the documentation about each function, then apply those functions to the data from the previous problem.\n",
    "\n",
    "sklearn.metrics.accuracy_score\n",
    "\n",
    "sklearn.metrics.precision_score\n",
    "\n",
    "sklearn.metrics.recall_score\n",
    "\n",
    "sklearn.metrics.classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d349bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "330356b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.689772</td>\n",
       "      <td>0.815006</td>\n",
       "      <td>0.747178</td>\n",
       "      <td>1746.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.890024</td>\n",
       "      <td>0.803319</td>\n",
       "      <td>0.844452</td>\n",
       "      <td>3254.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.807400</td>\n",
       "      <td>0.807400</td>\n",
       "      <td>0.807400</td>\n",
       "      <td>0.8074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.789898</td>\n",
       "      <td>0.809162</td>\n",
       "      <td>0.795815</td>\n",
       "      <td>5000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.820096</td>\n",
       "      <td>0.807400</td>\n",
       "      <td>0.810484</td>\n",
       "      <td>5000.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score    support\n",
       "cat            0.689772  0.815006  0.747178  1746.0000\n",
       "dog            0.890024  0.803319  0.844452  3254.0000\n",
       "accuracy       0.807400  0.807400  0.807400     0.8074\n",
       "macro avg      0.789898  0.809162  0.795815  5000.0000\n",
       "weighted avg   0.820096  0.807400  0.810484  5000.0000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model 1\")\n",
    "\n",
    "pd.DataFrame(classification_report(paws_df.actual, paws_df.model1,\n",
    "                                  labels =['cat','dog'],\n",
    "                                  output_dict=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0574f3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.484122</td>\n",
       "      <td>0.890607</td>\n",
       "      <td>0.627269</td>\n",
       "      <td>1746.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.893177</td>\n",
       "      <td>0.490781</td>\n",
       "      <td>0.633479</td>\n",
       "      <td>3254.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.630400</td>\n",
       "      <td>0.630400</td>\n",
       "      <td>0.630400</td>\n",
       "      <td>0.6304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.688649</td>\n",
       "      <td>0.690694</td>\n",
       "      <td>0.630374</td>\n",
       "      <td>5000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.750335</td>\n",
       "      <td>0.630400</td>\n",
       "      <td>0.631310</td>\n",
       "      <td>5000.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score    support\n",
       "cat            0.484122  0.890607  0.627269  1746.0000\n",
       "dog            0.893177  0.490781  0.633479  3254.0000\n",
       "accuracy       0.630400  0.630400  0.630400     0.6304\n",
       "macro avg      0.688649  0.690694  0.630374  5000.0000\n",
       "weighted avg   0.750335  0.630400  0.631310  5000.0000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model 2\")\n",
    "\n",
    "pd.DataFrame(classification_report(paws_df.actual, paws_df.model2,\n",
    "                                  labels =['cat','dog'],\n",
    "                                  output_dict=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d49aeb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.358347</td>\n",
       "      <td>0.511455</td>\n",
       "      <td>0.421425</td>\n",
       "      <td>1746.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.659888</td>\n",
       "      <td>0.508605</td>\n",
       "      <td>0.574453</td>\n",
       "      <td>3254.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.5096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.509118</td>\n",
       "      <td>0.510030</td>\n",
       "      <td>0.497939</td>\n",
       "      <td>5000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.554590</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.521016</td>\n",
       "      <td>5000.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score    support\n",
       "cat            0.358347  0.511455  0.421425  1746.0000\n",
       "dog            0.659888  0.508605  0.574453  3254.0000\n",
       "accuracy       0.509600  0.509600  0.509600     0.5096\n",
       "macro avg      0.509118  0.510030  0.497939  5000.0000\n",
       "weighted avg   0.554590  0.509600  0.521016  5000.0000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model 3\")\n",
    "\n",
    "pd.DataFrame(classification_report(paws_df.actual, paws_df.model3,\n",
    "                                  labels =['cat','dog'],\n",
    "                                  output_dict=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2186f636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.345361</td>\n",
       "      <td>0.483755</td>\n",
       "      <td>1746.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.731249</td>\n",
       "      <td>0.955747</td>\n",
       "      <td>0.828560</td>\n",
       "      <td>3254.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.742600</td>\n",
       "      <td>0.742600</td>\n",
       "      <td>0.742600</td>\n",
       "      <td>0.7426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.769239</td>\n",
       "      <td>0.650554</td>\n",
       "      <td>0.656157</td>\n",
       "      <td>5000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.757781</td>\n",
       "      <td>0.742600</td>\n",
       "      <td>0.708154</td>\n",
       "      <td>5000.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score    support\n",
       "cat            0.807229  0.345361  0.483755  1746.0000\n",
       "dog            0.731249  0.955747  0.828560  3254.0000\n",
       "accuracy       0.742600  0.742600  0.742600     0.7426\n",
       "macro avg      0.769239  0.650554  0.656157  5000.0000\n",
       "weighted avg   0.757781  0.742600  0.708154  5000.0000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model 4\")\n",
    "\n",
    "pd.DataFrame(classification_report(paws_df.actual, paws_df.model4,\n",
    "                                  labels =['cat','dog'],\n",
    "                                  output_dict=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "812dbbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a079a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4 baseline\n",
       "0    cat    cat    dog    cat    dog      cat\n",
       "1    dog    dog    cat    cat    dog      cat\n",
       "2    dog    cat    cat    cat    dog      cat\n",
       "3    dog    dog    dog    cat    dog      cat\n",
       "4    cat    cat    cat    dog    dog      cat"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5bd6638a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8900238338440586"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(paws_df['actual'], paws_df['model1'], pos_label='dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19d9c815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.803318992009834"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(paws_df['actual'], paws_df['model1'], pos_label='dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "de1624a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8074"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(paws_df['actual'], paws_df['model1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e5450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ec6d944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(predictions, positive='dog'):\n",
    "    return precision_score(paws_df.actual, predictions, pos_label=positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d6885c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(predictions, positive='dog'):\n",
    "    return precision_score(paws_df.actual, predictions, pos_label=positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "003b65ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, positive='dog'):\n",
    "    return precision_score(paws_df.actual, predictions, pos_label=positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "57a0ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification(predictions, positive='dog'):\n",
    "    return precision_score(paws_df.actual, predictions, pos_label=positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3fa0e28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>0.890024</td>\n",
       "      <td>0.890024</td>\n",
       "      <td>0.890024</td>\n",
       "      <td>0.890024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>0.893177</td>\n",
       "      <td>0.893177</td>\n",
       "      <td>0.893177</td>\n",
       "      <td>0.893177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>0.659888</td>\n",
       "      <td>0.659888</td>\n",
       "      <td>0.659888</td>\n",
       "      <td>0.659888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>0.731249</td>\n",
       "      <td>0.731249</td>\n",
       "      <td>0.731249</td>\n",
       "      <td>0.731249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            recall  precision  accuracy  classification\n",
       "model1    0.890024   0.890024  0.890024        0.890024\n",
       "model2    0.893177   0.893177  0.893177        0.893177\n",
       "model3    0.659888   0.659888  0.659888        0.659888\n",
       "model4    0.731249   0.731249  0.731249        0.731249\n",
       "baseline  0.000000   0.000000  0.000000        0.000000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([\n",
    "   paws_df.loc[:, 'model1':'baseline'].apply(calculate_recall).rename('recall'),\n",
    "   paws_df.loc[:, 'model1':'baseline'].apply(calculate_precision).rename('precision'),\n",
    "   paws_df.loc[:, 'model1':'baseline'].apply(calculate_accuracy).rename('accuracy'),\n",
    "   paws_df.loc[:, 'model1':'baseline'].apply(calculate_accuracy).rename('classification'), \n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "precision_score?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e869b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9bd01ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(predictions, positive='cat'):\n",
    "    return precision_score(paws_df.actual, predictions, pos_label=positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "beaaf933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(predictions, positive='cat'):\n",
    "    return precision_score(paws_df.actual, predictions, pos_label=positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f40e65ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, positive='cat'):\n",
    "    return precision_score(paws_df.actual, predictions, pos_label=positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7480b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification(predictions, positive='cat'):\n",
    "    return precision_score(paws_df.actual, predictions, pos_label=positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0680526d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>0.689772</td>\n",
       "      <td>0.689772</td>\n",
       "      <td>0.689772</td>\n",
       "      <td>0.689772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>0.484122</td>\n",
       "      <td>0.484122</td>\n",
       "      <td>0.484122</td>\n",
       "      <td>0.484122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>0.358347</td>\n",
       "      <td>0.358347</td>\n",
       "      <td>0.358347</td>\n",
       "      <td>0.358347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.807229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.349200</td>\n",
       "      <td>0.349200</td>\n",
       "      <td>0.349200</td>\n",
       "      <td>0.349200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            recall  precision  accuracy  classification\n",
       "model1    0.689772   0.689772  0.689772        0.689772\n",
       "model2    0.484122   0.484122  0.484122        0.484122\n",
       "model3    0.358347   0.358347  0.358347        0.358347\n",
       "model4    0.807229   0.807229  0.807229        0.807229\n",
       "baseline  0.349200   0.349200  0.349200        0.349200"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([\n",
    "   paws_df.loc[:, 'model1':'baseline'].apply(calculate_recall).rename('recall'),\n",
    "   paws_df.loc[:, 'model1':'baseline'].apply(calculate_precision).rename('precision'),\n",
    "   paws_df.loc[:, 'model1':'baseline'].apply(calculate_accuracy).rename('accuracy'),\n",
    "   paws_df.loc[:, 'model1':'baseline'].apply(calculate_accuracy).rename('classification'), \n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f212b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
